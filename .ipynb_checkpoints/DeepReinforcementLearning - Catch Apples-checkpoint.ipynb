{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "composed-morgan",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-17T11:08:18.098879Z",
     "start_time": "2021-06-17T11:08:18.096623Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.8.5\n"
     ]
    }
   ],
   "source": [
    "from platform import python_version\n",
    "print(python_version())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "shaped-framework",
   "metadata": {},
   "source": [
    "# Deep Reinforcement Learning\n",
    "The model learns from experience instead of labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "valuable-kitty",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-17T11:08:19.642949Z",
     "start_time": "2021-06-17T11:08:18.100243Z"
    }
   },
   "outputs": [],
   "source": [
    "!pip install -q -U watermark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "destroyed-literature",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-17T11:08:23.592298Z",
     "start_time": "2021-06-17T11:08:19.649126Z"
    }
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import json \n",
    "import time\n",
    "import numpy as np \n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt \n",
    "from PIL import Image\n",
    "from IPython import display \n",
    "import seaborn\n",
    "import tensorflow\n",
    "import keras\n",
    "from keras.models import model_from_json\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "\n",
    "%matplotlib inline \n",
    "seaborn.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "caring-construction",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-17T11:08:23.616321Z",
     "start_time": "2021-06-17T11:08:23.593509Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Author: Data Science Academy\n",
      "\n",
      "seaborn   : 0.11.1\n",
      "json      : 2.0.9\n",
      "keras     : 2.4.3\n",
      "IPython   : 7.21.0\n",
      "matplotlib: 3.3.4\n",
      "tensorflow: 2.4.1\n",
      "autopep8  : 1.5.6\n",
      "PIL       : 8.1.2\n",
      "numpy     : 1.19.5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Versões dos pacotes usados neste jupyter notebook\n",
    "%reload_ext watermark\n",
    "%watermark -a \"Data Science Academy\" --iversions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "handled-newsletter",
   "metadata": {},
   "source": [
    "## Preparando o game\n",
    "\n",
    "No jogo, frutas, representadas por azulejos brancos, caem do topo. O objetivo é pegar os frutos com um basket (representado por azulejos brancos). Se você pegar uma fruta, você obtém um ponto (sua pontuação sobe por um), se você perder uma fruta, perdeu um (sua pontuação diminui).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "touched-monday",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-17T11:08:23.624772Z",
     "start_time": "2021-06-17T11:08:23.617447Z"
    }
   },
   "outputs": [],
   "source": [
    "class Catch(object):\n",
    "    def __init__(self, grid_size = 10):\n",
    "        self.grid_size = grid_size\n",
    "        self.reset()\n",
    "    \n",
    "    def _update_state(self, action):\n",
    "        state = self.state\n",
    "        if action == 0: #left\n",
    "            action = -1\n",
    "        elif action == 1: #stay\n",
    "            action = 0\n",
    "        else:\n",
    "            action == 1 #right\n",
    "        \n",
    "        f0, f1, basket = state[0]\n",
    "        new_basket = min(max(1, basket + action), self.grid_size-1)\n",
    "        f0 += 1\n",
    "        out = np.asarray([f0, f1, new_basket])\n",
    "        out = out[np.newaxis]\n",
    "        \n",
    "        assert len(out.shape) == 2\n",
    "        self.state = out \n",
    "        \n",
    "    def _draw_state(self):\n",
    "        im_size = (self.grid_size,)*2\n",
    "        state = self.state[0]\n",
    "        canvas = np.zeros(im_size)\n",
    "        canvas[state[0], state[1]] = 1 # desenha fruta\n",
    "        #canvas[-1, state[2]-1:state[2] + 2] = 1  # desenha basket\n",
    "        return canvas\n",
    "    \n",
    "    def _get_reward(self):\n",
    "        fruit_row, fruit_col, basket = self.state[0]\n",
    "        if fruit_row == self.grid_size-1:\n",
    "            if abs(fruit_col - basket) <= 1:\n",
    "                return 1\n",
    "            else:\n",
    "                return -1\n",
    "        else:\n",
    "            return 0\n",
    "        \n",
    "    def _is_over(self):\n",
    "        if self.state[0,0] == self.grid_size - 1:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "        \n",
    "    def observe(self):\n",
    "        canvas = self._draw_state()\n",
    "        return canvas.reshape((1,-1))\n",
    "    \n",
    "    def act(self):\n",
    "        self._update_state(action)\n",
    "        reward = self._get_reward()\n",
    "        game_over = self._is_over()\n",
    "        return self.observe(), reward, game_over\n",
    "    \n",
    "    def reset(self):\n",
    "        n = np.random.randint(0, self.grid_size-1, size=1)\n",
    "        m = np.random.randint(1, self.grid_size-2, size=1)\n",
    "        self.state = np.asarray([0,n,m])[np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "crucial-knitting",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-17T11:10:18.021057Z",
     "start_time": "2021-06-17T11:10:18.018621Z"
    }
   },
   "outputs": [],
   "source": [
    "# Setando variavies do ambiente\n",
    "\n",
    "# o ultimo time frame faz o controle do quadro que estamos atualmente\n",
    "last_frame_time = 0 \n",
    "\n",
    "# Traduz as acoes para o vocabulario humano \n",
    "translate_action = [\"Left\", \"Stay\", \"Right\", \"Create Ball\", \"End Test\"]\n",
    "\n",
    "# Tamanho do campo de jogo\n",
    "grid_size = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "liable-munich",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-17T11:16:02.359303Z",
     "start_time": "2021-06-17T11:16:02.354497Z"
    }
   },
   "outputs": [],
   "source": [
    "# Definindo funcoes auxiliares para o ambiente\n",
    "def display_screen(action, points, input_t):\n",
    "    global last_frame_time\n",
    "    print(\"Action %s, Points: %s\" % (translate_action[action], points))\n",
    "    \n",
    "    # Somente ira mostrar a tela do jogo se nao for game over\n",
    "    if(\"End\" not in translate_action[action]):\n",
    "        plt.imshow(input_t.reshape((grid_size,)*2), interpolation='none', cmap='gray')\n",
    "        display.clear_output(wait=True)\n",
    "        display.display(plt.gcf())\n",
    "    last_frame_time = set_max_fps(last_frame_time)\n",
    "    \n",
    "def set_max_fps(last_frame_time, FPS=1):\n",
    "    current_milli_time = lambda: int(round(time.time()*1000))\n",
    "    sleep_time = 1./FPS - (current_milli_time() - last_frame_time)\n",
    "    if sleep_time > 0:\n",
    "        time.sleep(sleep_time)\n",
    "    return current_milli_time()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wireless-interim",
   "metadata": {},
   "source": [
    "## Treinando o agente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opened-webmaster",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExperienceReplay(object):\n",
    "    def __init__(self, max_memory=100, discount=0.9):\n",
    "        self.max_memory = max_memory\n",
    "        self.memory = list()\n",
    "        self.discount = discount\n",
    "        \n",
    "    def remember(self, states, game_over):\n",
    "        # Salvando o estado na memoria\n",
    "        self.memory.append([states, game_over])\n",
    "        \n",
    "        # Para nao lotar a memoria\n",
    "        if len(self.memory) > self.max_memory:\n",
    "            del self.memory[0]\n",
    "            \n",
    "    def get_batch(self, model, batch_size=10):\n",
    "        # Quantas experiencias temos\n",
    "        len_memory = len(self.memory)\n",
    "        \n",
    "        # Calcule o numero de acoes que podem ser tomadas no jogo\n",
    "        num_actions = model.output_shape[-1]\n",
    "        \n",
    "        # Dimensoes do campode jogo\n",
    "        env_dim = self.memory[0][0][0].shape[1]\n",
    "        \n",
    "        # Queremos retornar um vetor de entrada e destino com entradas de um estado observado\n",
    "        inputs = np.zeros((min(len_memory, batch_size), env_dim))\n",
    "        \n",
    "        # ... e target r + gamma * max Q(s', a')\n",
    "        # Observe que nosso alvo é uma matriz, com possíveis campos não só para a ação realizada, mas também \n",
    "        # para as outras ações possíveis. As ações não tomam o mesmo valor que a previsão de não afetá-las.\n",
    "        targets = np.zeros((inputs.shape[0], num_actions))\n",
    "        \n",
    "        # Nos desenhamos estados para aprender aleatoriamente\n",
    "        for i, idx, in enumerate(np.random.randint(0, len_memory, size=inputs.shape[0])):\n",
    "            \n",
    "            state_t, action_t, reward_t, state_tp1 = self.memory[idx][0]\n",
    "            \n",
    "          # Também precisamos saber se o jogo terminou nesse estado\n",
    "            game_over = self.memory[idx][1]\n",
    "            \n",
    "            # Adicione o estado s à entrada\n",
    "            inputs[i:i+1] = state_t\n",
    "            \n",
    "            # Primeiro, preenchemos os valores-alvo com as previsões do modelo. \n",
    "            # Eles não serão afetados pelo treinamento (uma vez que a perda de treinamento para eles é 0)\n",
    "            targets[i] = model.predict(state_t)[0]\n",
    "            \n",
    "            \"\"\"\n",
    "            Se o jogo acabou, a recompensa esperada Q (s, a) deve ser a recompensa final r.\n",
    "            Ou então o target value é r + gamma * max Q(s’,a’)\n",
    "            \"\"\"\n",
    "            \n",
    "            Q_sa = np.max(model.predict(state-tp1)[0])\n",
    "            \n",
    "          # Se o jogo acabou, a recompensa é a recompensa final.\n",
    "            if game_over:\n",
    "                targets[i, action_t] = reward_t\n",
    "            # r + gamma * max Q(s’,a’)\n",
    "            else:\n",
    "                targets[i, action_t] = reward_t + self.discount * Q_sa\n",
    "        return inputs, targets"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
